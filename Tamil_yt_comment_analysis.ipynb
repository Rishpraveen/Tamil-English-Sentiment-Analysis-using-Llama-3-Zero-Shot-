{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7X72hbgNITOxzqCtsnhRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishpraveen/Tamil-English-Sentiment-Analysis-using-Llama-3-Zero-Shot-/blob/main/Tamil_yt_comment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tamil YouTube Comment Sentiment Analysis - Fixed Version"
      ],
      "metadata": {
        "id": "qewvPtdg4cQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "fotNqoQx2GMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate torch bitsandbytes flash-attn --no-build-isolation\n",
        "!pip install google-api-python-client pandas matplotlib seaborn wordcloud ipywidgets\n",
        "!pip install huggingface_hub\n",
        "!pip install torchvision==0.18.1  # Explicitly install a compatible torchvision version"
      ],
      "metadata": {
        "id": "QIiZopgf6ChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 2: Import Librariesy"
      ],
      "metadata": {
        "id": "oawmp7J18YVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "M5V_W0XX-qxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformers and PyTorch\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# YouTube API\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# Hugging Face Hub & Colab\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Interactive Dashboard\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n"
      ],
      "metadata": {
        "id": "K_7ESbD92MAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 3: Configuration and Setup"
      ],
      "metadata": {
        "id": "xmibs5RY2aHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration class for the sentiment analysis project.\"\"\"\n",
        "    MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "    MAX_NEW_TOKENS = 5\n",
        "    TEMPERATURE = 0.1\n",
        "    TOP_P = 0.5\n",
        "    MAX_COMMENTS_TO_FETCH = 100\n",
        "    COMMENTS_PER_PAGE = 50\n",
        "    SENTIMENT_LABELS = {'Positive', 'Negative', 'Neutral'}\n",
        "    SENTIMENT_COLORS = {\n",
        "        \"Positive\": \"#4CAF50\", \"Negative\": \"#F44336\", \"Neutral\": \"#2196F3\",\n",
        "        \"Unknown\": \"#9E9E9E\", \"Error\": \"#FFC107\"\n",
        "    }\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU availability and specifications.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è GPU not available, using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = check_gpu()"
      ],
      "metadata": {
        "id": "psH44o3Z2YFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 4: Authentication Setup"
      ],
      "metadata": {
        "id": "ft5SG6H52lLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_authentication():\n",
        "    \"\"\"Setup Hugging Face and get YouTube API key.\"\"\"\n",
        "    try:\n",
        "        hf_token = userdata.get('HF_TOKEN')\n",
        "        print(\"üîë Authenticating with Hugging Face...\")\n",
        "        login(token=hf_token)\n",
        "        print(\"‚úÖ Hugging Face login successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Hugging Face login failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        api_key = userdata.get('YOUTUBE_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"YOUTUBE_API_KEY not found in Colab secrets.\")\n",
        "        print(\"‚úÖ YouTube API Key loaded successfully.\")\n",
        "        return api_key\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error retrieving YouTube API key: {e}\")\n",
        "        return None\n",
        "\n",
        "setup_authentication()"
      ],
      "metadata": {
        "id": "Oe3XM_Ok2jm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Cell 5: Model Loading and Setup"
      ],
      "metadata": {
        "id": "QZ9Cm4Cv2uDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalyzer:\n",
        "    \"\"\"Main sentiment analysis class using Llama 3 with robust fallbacks.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id=Config.MODEL_ID):\n",
        "        self.model_id = model_id\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.text_generator = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the Llama 3 model with optimizations.\"\"\"\n",
        "        try:\n",
        "            print(f\"üîÑ Loading model: {self.model_id}\")\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "            flash_attn_available = 'flash_attn' in globals()\n",
        "            attn_implementation = \"flash_attention_2\" if flash_attn_available and torch.cuda.is_available() else \"sdpa\"\n",
        "            if flash_attn_available:\n",
        "                 print(\"‚ö° Flash Attention 2 detected and will be used.\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_id,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                attn_implementation=attn_implementation\n",
        "            )\n",
        "            self.text_generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            print(\"‚úÖ Model and pipeline loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {e}\")\n",
        "            raise e # Re-raise the exception to stop execution and show the root cause\n",
        "\n",
        "    def _analyze_sentiment_fallback(self, text):\n",
        "        \"\"\"Robust keyword-based fallback for sentiment analysis.\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        positive_keywords = [\n",
        "            '‡Æ®‡Æ©‡Øç‡Æ±‡Ææ‡Æï', '‡ÆÖ‡Æ±‡Øç‡Æ™‡ØÅ‡Æ§‡ÆÆ‡Øç', '‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà', '‡Æö‡ØÇ‡Æ™‡Øç‡Æ™‡Æ∞‡Øç', '‡Æï‡Æ≤‡Æï‡Øç‡Æï‡Æ≤‡Øç', '‡Æ™‡Æø‡Æ∞‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç', '‡Æö‡Æ®‡Øç‡Æ§‡Øã‡Æ∑‡ÆÆ‡Øç',\n",
        "            '‡Æ™‡Æø‡Æü‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ', '‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§', '‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø', 'good', 'great', 'excellent',\n",
        "            'amazing', 'super', 'awesome', 'love', 'fantastic', 'best', 'nice', 'wow'\n",
        "        ]\n",
        "        negative_keywords = [\n",
        "            '‡ÆÆ‡Øã‡Æö‡ÆÆ‡Øç', '‡Æö‡Æ∞‡Æø‡ÆØ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà', '‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç', '‡Æï‡Øá‡Æµ‡Æ≤‡ÆÆ‡Øç', '‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡Øà', '‡Æµ‡Øá‡Æ∏‡Øç‡Æü‡Øç', '‡Æ®‡Øá‡Æ∞‡ÆÆ‡Øç ‡Æµ‡ØÄ‡Æ£‡Øç',\n",
        "            '‡Æè‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ÆÆ‡Øç', '‡Æö‡Øã‡Æï‡ÆÆ‡Øç', '‡Æï‡Øã‡Æ™‡ÆÆ‡Øç', 'bad', 'terrible', 'awful', 'waste',\n",
        "            'disappointing', 'hate', 'worst', 'boring', 'poor', 'sad'\n",
        "        ]\n",
        "\n",
        "        pos_score = sum(text_lower.count(word) for word in positive_keywords)\n",
        "        neg_score = sum(text_lower.count(word) for word in negative_keywords)\n",
        "\n",
        "        pos_score += text.count('üëç') + text.count('‚ù§Ô∏è') + text.count('üòç') + text.count('üî•')\n",
        "        neg_score += text.count('üëé') + text.count('üò†') + text.count('üò°')\n",
        "\n",
        "        if pos_score > neg_score: return \"Positive\"\n",
        "        if neg_score > pos_score: return \"Negative\"\n",
        "        return \"Neutral\"\n",
        "\n",
        "    def classify_sentiment(self, text):\n",
        "        \"\"\"Classify sentiment using Llama 3, with a fallback mechanism.\"\"\"\n",
        "        if not self.text_generator or not text or not text.strip():\n",
        "            return \"Neutral\"\n",
        "\n",
        "        messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are an expert at analyzing Tamil and Tamil-English (Tanglish) YouTube comments.\n",
        "Classify the sentiment as exactly one word: Positive, Negative, or Neutral.\n",
        "- YOUTUBE CONTEXT: Comments are informal, use mixed language, emojis (üëç‚ù§Ô∏èüî•=positive, üëéüò†=negative), and ALL CAPS.\n",
        "- TAMIL/TANGLISH PATTERNS: \"Super\", \"Arumai\", \"Nalla iruku\" = Positive. \"Mosam\", \"Waste\", \"Sariyilla\" = Negative. \"Paravalla\", \"Okay\" = Neutral.\n",
        "- YOUR TASK: Focus on the overall emotional tone. Respond with ONLY ONE WORD: Positive, Negative, or Neutral.\"\"\"\n",
        "        }, {\n",
        "            \"role\": \"user\", \"content\": f\"Sentiment of this comment: \\\"{text}\\\"\"\n",
        "        }]\n",
        "\n",
        "        try:\n",
        "            outputs = self.text_generator(\n",
        "                messages,\n",
        "                max_new_tokens=Config.MAX_NEW_TOKENS,\n",
        "                do_sample=False,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                top_p=Config.TOP_P,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "            response = outputs[0]['generated_text'][-1]['content'].strip().capitalize()\n",
        "\n",
        "            if response in Config.SENTIMENT_LABELS:\n",
        "                return response\n",
        "            else:\n",
        "                # If the response is not clean, parse it aggressively\n",
        "                response_lower = response.lower()\n",
        "                if 'positive' in response_lower: return \"Positive\"\n",
        "                if 'negative' in response_lower: return \"Negative\"\n",
        "                if 'neutral' in response_lower: return \"Neutral\"\n",
        "                return self._analyze_sentiment_fallback(text)\n",
        "        except Exception:\n",
        "            return self._analyze_sentiment_fallback(text)\n",
        "\n",
        "# Initialize analyzer globally to be used by the dashboard\n",
        "analyzer = SentimentAnalyzer()"
      ],
      "metadata": {
        "id": "UjfFRwXV2r6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 6: Test the Model"
      ],
      "metadata": {
        "id": "iv9-1QfW2zOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    \"\"\"Test the sentiment analysis model with sample texts\"\"\"\n",
        "\n",
        "    test_texts = [\n",
        "        \"‡Æá‡Æ®‡Øç‡Æ§ ‡Æ™‡Æü‡ÆÆ‡Øç ‡ÆÆ‡Æø‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ!\",  # Positive\n",
        "        \"‡Æö‡Øá‡Æµ‡Øà ‡ÆÆ‡Æø‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Øã‡Æö‡ÆÆ‡Øç, ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ§‡Æø ‡ÆÖ‡Æü‡Øà‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà.\",  # Negative\n",
        "        \"Super padam! Vera level acting.\",  # Code-mixed Positive\n",
        "        \"Waste of time and money. Highly disappointing.\",  # Negative\n",
        "        \"‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æí‡Æ∞‡ØÅ ‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ© ‡ÆÆ‡ØÅ‡Æü‡Øç‡Æü‡Ææ‡Æ≥‡Øç.\",  # Negative\n",
        "        \"‡Æµ‡Ææ‡Æ©‡Æø‡Æ≤‡Øà ‡Æá‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡Ææ‡Æ§‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ.\",  # Neutral\n",
        "        \"enna service idhu? very bad experience da.\",  # Code-mixed Negative\n",
        "        \"Decent attempt, but could be better.\",  # Neutral\n",
        "        \"‡Æ®‡Æ≤‡Øç‡Æ≤ ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø, ‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æá‡Æ©‡Øç‡Æ©‡ØÅ‡ÆÆ‡Øç ‡Æ®‡Æ©‡Øç‡Æ±‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æ≤‡Ææ‡ÆÆ‡Øç.\",  # Neutral\n",
        "        \"Absolutely fantastic! Best movie ever!\"  # Positive\n",
        "    ]\n",
        "\n",
        "    print(\"üß™ Testing Sentiment Analysis Model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    results = []\n",
        "    for i, text in enumerate(test_texts, 1):\n",
        "        print(f\"\\n{i}. Text: \\\"{text}\\\"\")\n",
        "        sentiment = analyzer.classify_sentiment(text)\n",
        "        print(f\"   Sentiment: {sentiment}\")\n",
        "        results.append({\"Text\": text, \"Sentiment\": sentiment})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run tests\n",
        "test_results = test_model()\n",
        "print(\"\\nüìä Test Results Summary:\")\n",
        "print(test_results['Sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "_mFUuyUi238F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 7: YouTube Comment Extraction"
      ],
      "metadata": {
        "id": "8Dmenhne27AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YouTubeCommentExtractor:\n",
        "    \"\"\"Extracts comments and video info from YouTube.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API Key is missing.\")\n",
        "        self.youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    def extract_video_id(self, url):\n",
        "        \"\"\"Extracts video ID from a YouTube URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        if parsed_url.hostname == 'youtu.be':\n",
        "            return parsed_url.path[1:]\n",
        "        if parsed_url.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "            if parsed_url.path == '/watch':\n",
        "                return parse_qs(parsed_url.query).get('v', [None])[0]\n",
        "        return None\n",
        "\n",
        "    def get_video_info(self, video_id):\n",
        "        \"\"\"Fetches video title, channel, and stats.\"\"\"\n",
        "        try:\n",
        "            request = self.youtube.videos().list(part=\"snippet,statistics\", id=video_id)\n",
        "            response = request.execute()\n",
        "            if not response[\"items\"]: return None\n",
        "            video = response[\"items\"][0]\n",
        "            stats = video.get(\"statistics\", {})\n",
        "            return {\n",
        "                \"title\": video[\"snippet\"][\"title\"],\n",
        "                \"channel\": video[\"snippet\"][\"channelTitle\"],\n",
        "                \"view_count\": f'{int(stats.get(\"viewCount\", 0)):,}',\n",
        "                \"comment_count\": f'{int(stats.get(\"commentCount\", 0)):,}'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not fetch video info: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_comments(self, video_id, max_comments):\n",
        "        \"\"\"Generator function to fetch comments page by page.\"\"\"\n",
        "        next_page_token = None\n",
        "        comments_fetched = 0\n",
        "        while comments_fetched < max_comments:\n",
        "            try:\n",
        "                request = self.youtube.commentThreads().list(\n",
        "                    part=\"snippet\",\n",
        "                    videoId=video_id,\n",
        "                    maxResults=min(Config.COMMENTS_PER_PAGE, max_comments - comments_fetched),\n",
        "                    textFormat=\"plainText\",\n",
        "                    pageToken=next_page_token\n",
        "                )\n",
        "                response = request.execute()\n",
        "\n",
        "                for item in response.get(\"items\", []):\n",
        "                    if comments_fetched >= max_comments: break\n",
        "                    comment_data = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "                    comment_text = re.sub(r'\\s+', ' ', comment_data[\"textDisplay\"]).strip()\n",
        "                    if comment_text:\n",
        "                        yield comment_text\n",
        "                        comments_fetched += 1\n",
        "\n",
        "                next_page_token = response.get(\"nextPageToken\")\n",
        "                if not next_page_token: break\n",
        "                time.sleep(0.2)\n",
        "            except HttpError as e:\n",
        "                raise e # Re-raise to be caught by the main analysis loop\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during comment fetching: {e}\")\n",
        "                break"
      ],
      "metadata": {
        "id": "6exdDA3o26tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 8: Sentiment Analysis Pipeline"
      ],
      "metadata": {
        "id": "kUEs7j4F3Amx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_youtube_video(video_url, max_comments=100):\n",
        "    \"\"\"Complete pipeline to analyze YouTube video sentiment\"\"\"\n",
        "\n",
        "    # Get API key\n",
        "    api_key = get_youtube_api_key()\n",
        "    if not api_key:\n",
        "        print(\"‚ùå YouTube API key not found\")\n",
        "        return None\n",
        "\n",
        "    # Initialize extractor\n",
        "    extractor = YouTubeCommentExtractor(api_key)\n",
        "\n",
        "    # Extract video ID\n",
        "    video_id = extractor.extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"‚ùå Could not extract video ID\")\n",
        "        return None\n",
        "\n",
        "    # Get video info\n",
        "    video_info = extractor.get_video_info(video_id)\n",
        "    if video_info:\n",
        "        print(f\"üì∫ Video: {video_info['title']}\")\n",
        "        print(f\"üì∫ Channel: {video_info['channel']}\")\n",
        "        print(f\"üëÅÔ∏è  Views: {video_info['view_count']}\")\n",
        "        print(f\"üí¨ Comments: {video_info['comment_count']}\")\n",
        "\n",
        "    # Extract comments\n",
        "    comments = extractor.extract_comments(video_id, max_comments)\n",
        "\n",
        "    if not comments:\n",
        "        print(\"‚ùå No comments found\")\n",
        "        return None\n",
        "\n",
        "    # Analyze sentiment\n",
        "    print(f\"\\nüîÑ Analyzing sentiment for {len(comments)} comments...\")\n",
        "\n",
        "    results = []\n",
        "    for i, comment in enumerate(comments, 1):\n",
        "        print(f\"Analyzing comment {i}/{len(comments)}: {comment['comment'][:50]}...\")\n",
        "\n",
        "        sentiment = analyzer.classify_sentiment(comment['comment'])\n",
        "\n",
        "        results.append({\n",
        "            'Author': comment['author'],\n",
        "            'Comment': comment['comment'],\n",
        "            'Published_At': comment['published_at'],\n",
        "            'Likes': comment['like_count'],\n",
        "            'Replies': comment['reply_count'],\n",
        "            'Sentiment': sentiment,\n",
        "            'Comment_Length': len(comment['comment'])\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df, video_info"
      ],
      "metadata": {
        "id": "ksMNSCOn3Fau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 9: Visualization Functions"
      ],
      "metadata": {
        "id": "wRZTX6943JbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentVisualizer:\n",
        "    \"\"\"Visualization class for sentiment analysis results\"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.colors = Config.COLORS\n",
        "\n",
        "    def plot_sentiment_distribution(self):\n",
        "        \"\"\"Plot sentiment distribution\"\"\"\n",
        "        plt.figure(figsize=Config.FIGSIZE)\n",
        "\n",
        "        sentiment_counts = self.df['Sentiment'].value_counts()\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        sentiment_counts.plot(kind='bar', color=self.colors)\n",
        "        plt.title('Sentiment Distribution')\n",
        "        plt.xlabel('Sentiment')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.pie(sentiment_counts.values, labels=sentiment_counts.index,\n",
        "                autopct='%1.1f%%', colors=self.colors)\n",
        "        plt.title('Sentiment Distribution (Pie Chart)')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_sentiment_over_time(self):\n",
        "        \"\"\"Plot sentiment over time\"\"\"\n",
        "        plt.figure(figsize=Config.FIGSIZE)\n",
        "\n",
        "        # Convert timestamp to datetime\n",
        "        self.df['Published_At'] = pd.to_datetime(self.df['Published_At'])\n",
        "\n",
        "        # Group by date and sentiment\n",
        "        daily_sentiment = self.df.groupby([\n",
        "            self.df['Published_At'].dt.date, 'Sentiment'\n",
        "        ]).size().unstack(fill_value=0)\n",
        "\n",
        "        daily_sentiment.plot(kind='line', marker='o', figsize=(12, 6))\n",
        "        plt.title('Sentiment Trends Over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Number of Comments')\n",
        "        plt.legend(title='Sentiment')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_engagement_analysis(self):\n",
        "        \"\"\"Plot engagement analysis\"\"\"\n",
        "        plt.figure(figsize=Config.FIGSIZE)\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        self.df.boxplot(column='Likes', by='Sentiment', ax=plt.gca())\n",
        "        plt.title('Likes by Sentiment')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        self.df.boxplot(column='Comment_Length', by='Sentiment', ax=plt.gca())\n",
        "        plt.title('Comment Length by Sentiment')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        avg_likes = self.df.groupby('Sentiment')['Likes'].mean()\n",
        "        avg_likes.plot(kind='bar', color=self.colors)\n",
        "        plt.title('Average Likes by Sentiment')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        avg_length = self.df.groupby('Sentiment')['Comment_Length'].mean()\n",
        "        avg_length.plot(kind='bar', color=self.colors)\n",
        "        plt.title('Average Comment Length by Sentiment')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_word_cloud(self):\n",
        "        \"\"\"Generate word clouds for each sentiment\"\"\"\n",
        "        sentiments = self.df['Sentiment'].unique()\n",
        "\n",
        "        fig, axes = plt.subplots(1, len(sentiments), figsize=(15, 5))\n",
        "        if len(sentiments) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, sentiment in enumerate(sentiments):\n",
        "            comments = self.df[self.df['Sentiment'] == sentiment]['Comment']\n",
        "            text = ' '.join(comments)\n",
        "\n",
        "            wordcloud = WordCloud(width=400, height=200,\n",
        "                                background_color='white',\n",
        "                                colormap='viridis').generate(text)\n",
        "\n",
        "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
        "            axes[i].set_title(f'{sentiment} Comments')\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_summary_report(self, video_info=None):\n",
        "        \"\"\"Generate a summary report\"\"\"\n",
        "        total_comments = len(self.df)\n",
        "        sentiment_counts = self.df['Sentiment'].value_counts()\n",
        "\n",
        "        print(\"üìä SENTIMENT ANALYSIS REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        if video_info:\n",
        "            print(f\"üì∫ Video: {video_info['title']}\")\n",
        "            print(f\"üì∫ Channel: {video_info['channel']}\")\n",
        "            print(f\"üëÅÔ∏è  Views: {video_info['view_count']}\")\n",
        "            print()\n",
        "\n",
        "        print(f\"üí¨ Total Comments Analyzed: {total_comments}\")\n",
        "        print(f\"üìÖ Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print()\n",
        "\n",
        "        print(\"üìà Sentiment Distribution:\")\n",
        "        for sentiment, count in sentiment_counts.items():\n",
        "            percentage = (count / total_comments) * 100\n",
        "            print(f\"   {sentiment}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        print()\n",
        "        print(\"üîç Key Insights:\")\n",
        "\n",
        "        # Most liked comment by sentiment\n",
        "        for sentiment in sentiment_counts.index:\n",
        "            most_liked = self.df[self.df['Sentiment'] == sentiment].nlargest(1, 'Likes')\n",
        "            if not most_liked.empty:\n",
        "                print(f\"   Most liked {sentiment.lower()} comment ({most_liked.iloc[0]['Likes']} likes):\")\n",
        "                print(f\"   \\\"{most_liked.iloc[0]['Comment'][:100]}...\\\"\")\n",
        "\n",
        "        # Average engagement\n",
        "        avg_likes = self.df.groupby('Sentiment')['Likes'].mean()\n",
        "        print(f\"\\nüìä Average Likes by Sentiment:\")\n",
        "        for sentiment, avg in avg_likes.items():\n",
        "            print(f\"   {sentiment}: {avg:.1f} likes\")"
      ],
      "metadata": {
        "id": "_lYsCSPs3LS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 10: Main Execution"
      ],
      "metadata": {
        "id": "Sh3ehT113QIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Example YouTube video URL (replace with your desired video)\n",
        "    video_url = input(\"Enter YouTube video URL: \")\n",
        "    max_comments = int(input(\"Enter maximum number of comments to analyze (default 100): \") or 100)\n",
        "\n",
        "    print(f\"\\nüöÄ Starting sentiment analysis for: {video_url}\")\n",
        "    print(f\"üìä Maximum comments to analyze: {max_comments}\")\n",
        "\n",
        "    # Analyze video\n",
        "    result = analyze_youtube_video(video_url, max_comments)\n",
        "\n",
        "    if result is None:\n",
        "        print(\"‚ùå Analysis failed\")\n",
        "        return\n",
        "\n",
        "    df, video_info = result\n",
        "\n",
        "    # Create visualizer\n",
        "    visualizer = SentimentVisualizer(df)\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"\\nüìä Generating visualizations...\")\n",
        "    visualizer.plot_sentiment_distribution()\n",
        "    visualizer.plot_engagement_analysis()\n",
        "    visualizer.generate_word_cloud()\n",
        "\n",
        "    # Generate summary report\n",
        "    visualizer.generate_summary_report(video_info)\n",
        "\n",
        "    # Save results\n",
        "    output_file = f\"sentiment_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nüíæ Results saved to: {output_file}\")\n",
        "\n",
        "    return df, video_info"
      ],
      "metadata": {
        "id": "elrdE9IO3Srx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 11: Interactive Analysis"
      ],
      "metadata": {
        "id": "CLD91wPw3ebm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_interactive_dashboard():\n",
        "    \"\"\"Launches the full interactive sentiment analysis dashboard.\"\"\"\n",
        "\n",
        "    # --- Widget Creation ---\n",
        "    url_input = widgets.Text(\n",
        "        value='https://www.youtube.com/watch?v=R4aT6v_L-3E', # Example Tamil video\n",
        "        description='Video URL:',\n",
        "        layout=widgets.Layout(width='600px'),\n",
        "        style={'description_width': '100px'}\n",
        "    )\n",
        "    max_comments_slider = widgets.IntSlider(\n",
        "        value=50, min=10, max=500, step=10, description='Max Comments:',\n",
        "        layout=widgets.Layout(width='400px'), style={'description_width': '100px'}\n",
        "    )\n",
        "    analyze_button = widgets.Button(description='Start Analysis', button_style='success', icon='play')\n",
        "    stop_button = widgets.Button(description='Stop Analysis', button_style='danger', icon='stop')\n",
        "    clear_button = widgets.Button(description='Clear', button_style='info', icon='refresh')\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    # Global state for analysis thread\n",
        "    analysis_globals = {'is_running': False}\n",
        "\n",
        "    def start_analysis_clicked(b):\n",
        "        \"\"\"Event handler for the start button.\"\"\"\n",
        "        if analysis_globals['is_running']:\n",
        "            with output_area:\n",
        "                print(\"Analysis is already in progress.\")\n",
        "            return\n",
        "\n",
        "        analysis_globals['is_running'] = True\n",
        "        output_area.clear_output()\n",
        "\n",
        "        with output_area:\n",
        "            run_analysis_logic(\n",
        "                url_input.value,\n",
        "                max_comments_slider.value,\n",
        "                analysis_globals\n",
        "            )\n",
        "\n",
        "    def stop_analysis_clicked(b):\n",
        "        \"\"\"Event handler for the stop button.\"\"\"\n",
        "        if analysis_globals['is_running']:\n",
        "            analysis_globals['is_running'] = False\n",
        "            with output_area:\n",
        "                display(HTML(\"<div style='color: #e65100; font-weight: bold;'>üõë Analysis stopping...</div>\"))\n",
        "\n",
        "    def clear_clicked(b):\n",
        "        \"\"\"Clears the output area.\"\"\"\n",
        "        output_area.clear_output()\n",
        "        analysis_globals['is_running'] = False\n",
        "\n",
        "    analyze_button.on_click(start_analysis_clicked)\n",
        "    stop_button.on_click(stop_analysis_clicked)\n",
        "    clear_button.on_click(clear_clicked)\n",
        "\n",
        "    # --- UI Display ---\n",
        "    intro_html = \"\"\"\n",
        "    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 15px; text-align: center;'>\n",
        "        <h1 style='font-size: 28px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);'>üé¨ Tamil YouTube Sentiment Analyzer</h1>\n",
        "        <p style='font-size: 16px; opacity: 0.9;'>Enter a video URL to discover the public sentiment!</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(intro_html))\n",
        "    display(widgets.VBox([\n",
        "        url_input,\n",
        "        max_comments_slider,\n",
        "        widgets.HBox([analyze_button, stop_button, clear_button]),\n",
        "        output_area\n",
        "    ]))\n",
        "\n",
        "\n",
        "def run_analysis_logic(video_url, max_comments, analysis_globals):\n",
        "    \"\"\"The core logic that fetches, analyzes, and displays results.\"\"\"\n",
        "\n",
        "    api_key = userdata.get('YOUTUBE_API_KEY')\n",
        "    if not api_key:\n",
        "        display(HTML(\"<div style='color: red; font-weight: bold;'>Error: YouTube API Key is not configured.</div>\"))\n",
        "        analysis_globals['is_running'] = False\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        extractor = YouTubeCommentExtractor(api_key)\n",
        "        video_id = extractor.extract_video_id(video_url)\n",
        "        if not video_id:\n",
        "            display(HTML(f\"<div style='color: red; font-weight: bold;'>Error: Could not extract video ID from '{video_url}'.</div>\"))\n",
        "            analysis_globals['is_running'] = False\n",
        "            return\n",
        "\n",
        "        video_info = extractor.get_video_info(video_id)\n",
        "        if video_info:\n",
        "            info_html = f\"\"\"<div style='background: #f0f4f8; border-left: 5px solid #1976d2; padding: 15px; margin: 10px 0; border-radius: 5px;'>\n",
        "                <p><strong>Video:</strong> {video_info['title']}</p>\n",
        "                <p><strong>Channel:</strong> {video_info['channel']} | <strong>Views:</strong> {video_info['view_count']}</p>\n",
        "            </div>\"\"\"\n",
        "            display(HTML(info_html))\n",
        "\n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        display(HTML(f\"<h4>üîÑ Analyzing up to {max_comments} comments...</h4>\"))\n",
        "\n",
        "        for i, comment_text in enumerate(extractor.fetch_comments(video_id, max_comments)):\n",
        "            if not analysis_globals['is_running']:\n",
        "                display(HTML(\"<h4>Analysis stopped by user.</h4>\"))\n",
        "                break\n",
        "\n",
        "            sentiment = analyzer.classify_sentiment(comment_text)\n",
        "            results.append({'Comment': comment_text, 'Sentiment': sentiment})\n",
        "\n",
        "            # Real-time update\n",
        "            color = Config.SENTIMENT_COLORS.get(sentiment, '#9E9E9E')\n",
        "            display_text = comment_text[:120] + '...' if len(comment_text) > 120 else comment_text\n",
        "            progress_html = f\"\"\"<div style='border-left: 4px solid {color}; padding: 8px; margin: 4px 0; background: #fafafa;'>\n",
        "                <span style='color: {color}; font-weight: bold;'>{sentiment}</span>: <em>\"{display_text}\"</em>\n",
        "            </div>\"\"\"\n",
        "            display(HTML(progress_html))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        display_final_results(results, video_info, total_time)\n",
        "\n",
        "    except HttpError as e:\n",
        "        error_content = e.content.decode(\"utf-8\")\n",
        "        error_details = json.loads(error_content).get(\"error\", {})\n",
        "        error_html = f\"\"\"<div style='color: red; border: 1px solid red; padding: 10px; margin: 10px 0;'>\n",
        "            <strong>YouTube API Error ({e.resp.status}):</strong> {error_details.get('message', 'Unknown error.')}\n",
        "            <p>This often means your daily API quota is exceeded or the API key is invalid/restricted.</p>\n",
        "        </div>\"\"\"\n",
        "        display(HTML(error_html))\n",
        "    except Exception as e:\n",
        "        display(HTML(f\"<div style='color: red; font-weight: bold;'>An unexpected error occurred: {e}</div>\"))\n",
        "    finally:\n",
        "        analysis_globals['is_running'] = False\n",
        "\n",
        "\n",
        "def display_final_results(results, video_info, total_time):\n",
        "    \"\"\"Renders the final dashboard with charts and summaries.\"\"\"\n",
        "    if not results:\n",
        "        display(HTML(\"<h4>No comments were analyzed.</h4>\"))\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "    # --- Visualization ---\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Sentiment Analysis Dashboard', fontsize=20, fontweight='bold')\n",
        "\n",
        "    # Pie Chart\n",
        "    colors = [Config.SENTIMENT_COLORS.get(s, '#9E9E9E') for s in sentiment_counts.index]\n",
        "    ax1.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
        "            startangle=90, colors=colors, wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
        "    ax1.set_title('Sentiment Distribution', fontsize=16)\n",
        "\n",
        "    # Bar Chart\n",
        "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=colors, ax=ax2, hue=sentiment_counts.index, legend=False)\n",
        "    ax2.set_title('Sentiment Counts', fontsize=16)\n",
        "    ax2.set_ylabel('Number of Comments')\n",
        "    ax2.set_xlabel('Sentiment')\n",
        "    for container in ax2.containers:\n",
        "        ax2.bar_label(container)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # --- HTML Summary ---\n",
        "    total_comments = len(df)\n",
        "    summary_html = f\"\"\"\n",
        "    <div style='background: #e3f2fd; border: 1px solid #90caf9; padding: 20px; margin-top: 20px; border-radius: 10px;'>\n",
        "        <h3>üìä Analysis Summary</h3>\n",
        "        <p><strong>Total Comments Analyzed:</strong> {total_comments}</p>\n",
        "        <p><strong>Total Time Taken:</strong> {total_time:.2f} seconds</p>\n",
        "        <p><strong>Average Time per Comment:</strong> {total_time/total_comments:.2f} seconds</p>\n",
        "    </div>\n",
        "    <div style='margin-top:20px'>\n",
        "        <h3>üí¨ Comment Examples</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
        "        examples = df[df['Sentiment'] == sentiment].head(3)\n",
        "        if not examples.empty:\n",
        "            color = Config.SENTIMENT_COLORS[sentiment]\n",
        "            summary_html += f\"<h4 style='color:{color};'>{sentiment} Comments</h4>\"\n",
        "            for _, row in examples.iterrows():\n",
        "                summary_html += f\"<div style='border-left: 3px solid {color}; padding-left: 10px; margin-bottom: 8px;'><em>{row['Comment']}</em></div>\"\n",
        "\n",
        "    summary_html += \"</div>\"\n",
        "    display(HTML(summary_html))\n",
        "\n",
        "    # Save results to a file\n",
        "    output_file = f\"sentiment_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    display(HTML(f\"<p>üíæ Results saved to <strong>{output_file}</strong></p>\"))\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "run_interactive_dashboard()"
      ],
      "metadata": {
        "id": "u1Zmp1rw3iPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 12: Batch Processing Functions"
      ],
      "metadata": {
        "id": "Vz0jHOZ63xq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_analyze_videos(video_urls, max_comments_per_video=50):\n",
        "    \"\"\"Batch analyze multiple videos\"\"\"\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for i, url in enumerate(video_urls, 1):\n",
        "        print(f\"\\nüîÑ Processing video {i}/{len(video_urls)}: {url}\")\n",
        "\n",
        "        result = analyze_youtube_video(url, max_comments_per_video)\n",
        "        if result:\n",
        "            df, video_info = result\n",
        "            df['Video_URL'] = url\n",
        "            df['Video_Title'] = video_info['title'] if video_info else 'Unknown'\n",
        "            df['Video_Channel'] = video_info['channel'] if video_info else 'Unknown'\n",
        "            all_results.append(df)\n",
        "\n",
        "        time.sleep(2)  # Rate limiting\n",
        "\n",
        "    if all_results:\n",
        "        combined_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "        print(f\"\\nüìä Batch Analysis Complete!\")\n",
        "        print(f\"Total videos analyzed: {len(video_urls)}\")\n",
        "        print(f\"Total comments analyzed: {len(combined_df)}\")\n",
        "\n",
        "        # Save combined results\n",
        "        output_file = f\"batch_sentiment_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        combined_df.to_csv(output_file, index=False)\n",
        "        print(f\"üíæ Results saved to: {output_file}\")\n",
        "\n",
        "        # Generate summary\n",
        "        print(\"\\nüìà Overall Sentiment Distribution:\")\n",
        "        print(combined_df['Sentiment'].value_counts())\n",
        "\n",
        "        # Per-video summary\n",
        "        print(\"\\nüì∫ Per-Video Summary:\")\n",
        "        video_summary = combined_df.groupby('Video_Title')['Sentiment'].value_counts().unstack(fill_value=0)\n",
        "        print(video_summary)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "# video_urls = [\n",
        "#     \"https://youtu.be/VIDEO_ID_1\",\n",
        "#     \"https://youtu.be/VIDEO_ID_2\",\n",
        "#     \"https://youtu.be/VIDEO_ID_3\"\n",
        "# ]\n",
        "# batch_results = batch_analyze_videos(video_urls)\n",
        "\n",
        "print(\"‚úÖ Tamil-English Sentiment Analysis Tool Ready!\")\n",
        "print(\"üìã Available functions:\")\n",
        "print(\"   - main(): Run complete analysis\")\n",
        "print(\"   - interactive_analysis(): Interactive mode\")\n",
        "print(\"   - batch_analyze_videos(): Batch processing\")\n",
        "print(\"   - test_model(): Test with sample texts\")"
      ],
      "metadata": {
        "id": "j0iZEaDq315P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}