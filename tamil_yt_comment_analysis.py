# -*- coding: utf-8 -*-
"""Tamil_yt_comment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aTOcekBMIKzIl4YoT_y5z-fbo4MYF7u3

### Cell 1: Install Required Libraries
"""

!pip install --upgrade transformers accelerate torch bitsandbytes
!pip install google-api-python-client pandas matplotlib seaborn wordcloud
!pip install huggingface_hub datasets evaluate
!pip install --quiet textblob langdetect

### Cell 2: Import Librariesy

import os
import re
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# Transformers and PyTorch
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
    BitsAndBytesConfig
)

# YouTube API
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from urllib.parse import urlparse, parse_qs

# Hugging Face Hub
from huggingface_hub import login
from google.colab import userdata

"""### Cell 3: Configuration and Setup"""

class Config:
    """Configuration class for the sentiment analysis project"""

    # Model Configuration
    MODEL_ID = "meta-llama/Llama-3.2-1B-Instruct"
    MAX_NEW_TOKENS = 10
    TEMPERATURE = 0.1
    TOP_P = 0.9

    # YouTube API Configuration
    MAX_COMMENTS_TO_FETCH = 100
    COMMENTS_PER_PAGE = 50

    # Visualization Configuration
    FIGSIZE = (12, 8)
    COLORS = ['#ff9999', '#66b3ff', '#99ff99']

    # Sentiment Labels
    SENTIMENT_LABELS = {
        'positive': 'Positive',
        'negative': 'Negative',
        'neutral': 'Neutral'
    }

    # Check GPU availability
def check_gpu():
    """Check GPU availability and specifications"""
    if torch.cuda.is_available():
        print(f"✅ GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"📊 VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        return torch.device("cuda")
    else:
        print("⚠️  GPU not available, using CPU")
        return torch.device("cpu")

device = check_gpu()

"""### Cell 4: Authentication Setup"""

def setup_authentication():
    """Setup Hugging Face authentication"""
    try:
        # Try to get token from Colab secrets
        hf_token = userdata.get('HF_TOKEN')
        if hf_token:
            print("🔑 Using Hugging Face token from Colab secrets")
            login(token=hf_token)
        else:
            print("🔑 Please login to Hugging Face")
            login()
        return True
    except Exception as e:
        print(f"❌ Authentication failed: {e}")
        return False

def get_youtube_api_key():
    """Get YouTube API key from Colab secrets"""
    try:
        api_key = userdata.get('YOUTUBE_API_KEY')
        if not api_key:
            raise ValueError("YouTube API Key not found in Colab secrets")
        return api_key
    except Exception as e:
        print(f"❌ Error retrieving YouTube API key: {e}")
        return None

# Setup authentication
setup_authentication()

"""
### Cell 5: Model Loading and Setup"""

class SentimentAnalyzer:
    """Main sentiment analysis class using Llama 3"""

    def __init__(self, model_id=Config.MODEL_ID):
        self.model_id = model_id
        self.tokenizer = None
        self.model = None
        self.text_generator = None
        self.load_model()

    def load_model(self):
        """Load the Llama 3 model and tokenizer"""
        try:
            print(f"🔄 Loading model: {self.model_id}")

            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)

            # Configure quantization for memory efficiency
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )

            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.bfloat16
            )

            # Create text generation pipeline
            self.text_generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )

            print("✅ Model loaded successfully!")

        except Exception as e:
            print(f"❌ Error loading model: {e}")
            raise

    def classify_sentiment(self, text):
        """Classify sentiment of given text"""
        if not self.text_generator:
            raise RuntimeError("Model not loaded properly")

        # Prepare messages for chat format
        messages = [
            {
                "role": "system",
                "content": """You are an expert sentiment analysis assistant specializing in Tamil and Tamil-English code-mixed text.
                Your task is to classify the sentiment as strictly one of: Positive, Negative, or Neutral.
                Pay attention to the overall meaning and context. Respond with only the sentiment label."""
            },
            {
                "role": "user",
                "content": f"Analyze the sentiment of this text: \"{text}\"\n\nSentiment:"
            }
        ]

        try:
            # Generate response
            outputs = self.text_generator(
                messages,
                max_new_tokens=Config.MAX_NEW_TOKENS,
                do_sample=False,
                temperature=Config.TEMPERATURE,
                top_p=Config.TOP_P,
                pad_token_id=self.tokenizer.eos_token_id
            )

            # Extract response
            generated_text = outputs[0]['generated_text']
            assistant_response = self._extract_response(generated_text, text)

            # Parse sentiment
            sentiment = self._parse_sentiment(assistant_response)
            return sentiment

        except Exception as e:
            print(f"❌ Error in sentiment classification: {e}")
            return "Error"

    def _extract_response(self, generated_text, original_text):
        """Extract the assistant's response from generated text"""
        if isinstance(generated_text, list):
            for msg in reversed(generated_text):
                if msg.get("role") == "assistant":
                    return msg.get("content", "").strip()
        elif isinstance(generated_text, str):
            parts = generated_text.split("Sentiment:")
            if len(parts) > 1:
                return parts[-1].strip()
        return str(generated_text)

    def _parse_sentiment(self, response):
        """Parse sentiment from model response"""
        response_lower = response.lower()

        if re.search(r'\bpositive\b', response_lower):
            return "Positive"
        elif re.search(r'\bnegative\b', response_lower):
            return "Negative"
        elif re.search(r'\bneutral\b', response_lower):
            return "Neutral"
        else:
            print(f"⚠️  Could not parse sentiment from: '{response}'")
            return "Unknown"

# Initialize the analyzer
analyzer = SentimentAnalyzer()

"""### Cell 6: Test the Model"""

def test_model():
    """Test the sentiment analysis model with sample texts"""

    test_texts = [
        "இந்த படம் மிகவும் அருமையாக இருந்தது!",  # Positive
        "சேவை மிகவும் மோசம், நான் திருப்தி அடையவில்லை.",  # Negative
        "Super padam! Vera level acting.",  # Code-mixed Positive
        "Waste of time and money. Highly disappointing.",  # Negative
        "அவன் ஒரு சரியான முட்டாள்.",  # Negative
        "வானிலை இன்று சாதாரணமாக உள்ளது.",  # Neutral
        "enna service idhu? very bad experience da.",  # Code-mixed Negative
        "Decent attempt, but could be better.",  # Neutral
        "நல்ல முயற்சி, ஆனால் இன்னும் நன்றாக இருக்கலாம்.",  # Neutral
        "Absolutely fantastic! Best movie ever!"  # Positive
    ]

    print("🧪 Testing Sentiment Analysis Model")
    print("=" * 50)

    results = []
    for i, text in enumerate(test_texts, 1):
        print(f"\n{i}. Text: \"{text}\"")
        sentiment = analyzer.classify_sentiment(text)
        print(f"   Sentiment: {sentiment}")
        results.append({"Text": text, "Sentiment": sentiment})

    return pd.DataFrame(results)

# Run tests
test_results = test_model()
print("\n📊 Test Results Summary:")
print(test_results['Sentiment'].value_counts())

"""### Cell 7: YouTube Comment Extraction"""

class YouTubeCommentExtractor:
    """Extract comments from YouTube videos"""

    def __init__(self, api_key):
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)

    def extract_video_id(self, url):
        """Extract video ID from YouTube URL"""
        parsed_url = urlparse(url)

        if parsed_url.hostname == 'youtu.be':
            return parsed_url.path[1:]
        elif parsed_url.hostname in ('www.youtube.com', 'youtube.com'):
            if parsed_url.path == '/watch':
                return parse_qs(parsed_url.query).get('v', [None])[0]
            elif parsed_url.path.startswith('/embed/'):
                return parsed_url.path.split('/')[2]
            elif parsed_url.path.startswith('/v/'):
                return parsed_url.path.split('/')[2]

        print(f"⚠️  Could not extract video ID from: {url}")
        return None

    def get_video_info(self, video_id):
        """Get video information"""
        try:
            request = self.youtube.videos().list(
                part="snippet,statistics",
                id=video_id
            )
            response = request.execute()

            if response['items']:
                video = response['items'][0]
                return {
                    'title': video['snippet']['title'],
                    'channel': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': video['statistics'].get('viewCount', 0),
                    'like_count': video['statistics'].get('likeCount', 0),
                    'comment_count': video['statistics'].get('commentCount', 0)
                }
        except Exception as e:
            print(f"❌ Error getting video info: {e}")
        return None

    def extract_comments(self, video_id, max_comments=Config.MAX_COMMENTS_TO_FETCH):
        """Extract comments from a YouTube video"""
        comments = []
        next_page_token = None

        try:
            print(f"🔄 Extracting comments for video ID: {video_id}")

            while len(comments) < max_comments:
                request = self.youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=min(Config.COMMENTS_PER_PAGE, max_comments - len(comments)),
                    textFormat="plainText",
                    pageToken=next_page_token
                )

                response = request.execute()

                for item in response.get("items", []):
                    comment_data = item["snippet"]["topLevelComment"]["snippet"]

                    # Clean comment text
                    comment_text = re.sub(r'\s+', ' ', comment_data["textDisplay"]).strip()

                    if comment_text:  # Skip empty comments
                        comments.append({
                            'author': comment_data["authorDisplayName"],
                            'comment': comment_text,
                            'published_at': comment_data["publishedAt"],
                            'like_count': comment_data["likeCount"],
                            'reply_count': item["snippet"]["totalReplyCount"]
                        })

                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break

                time.sleep(0.5)  # Rate limiting

        except HttpError as e:
            print(f"❌ HTTP Error: {e}")
        except Exception as e:
            print(f"❌ Unexpected error: {e}")

        print(f"✅ Extracted {len(comments)} comments")
        return comments

"""### Cell 8: Sentiment Analysis Pipeline"""

def analyze_youtube_video(video_url, max_comments=100):
    """Complete pipeline to analyze YouTube video sentiment"""

    # Get API key
    api_key = get_youtube_api_key()
    if not api_key:
        print("❌ YouTube API key not found")
        return None

    # Initialize extractor
    extractor = YouTubeCommentExtractor(api_key)

    # Extract video ID
    video_id = extractor.extract_video_id(video_url)
    if not video_id:
        print("❌ Could not extract video ID")
        return None

    # Get video info
    video_info = extractor.get_video_info(video_id)
    if video_info:
        print(f"📺 Video: {video_info['title']}")
        print(f"📺 Channel: {video_info['channel']}")
        print(f"👁️  Views: {video_info['view_count']}")
        print(f"💬 Comments: {video_info['comment_count']}")

    # Extract comments
    comments = extractor.extract_comments(video_id, max_comments)

    if not comments:
        print("❌ No comments found")
        return None

    # Analyze sentiment
    print(f"\n🔄 Analyzing sentiment for {len(comments)} comments...")

    results = []
    for i, comment in enumerate(comments, 1):
        print(f"Analyzing comment {i}/{len(comments)}: {comment['comment'][:50]}...")

        sentiment = analyzer.classify_sentiment(comment['comment'])

        results.append({
            'Author': comment['author'],
            'Comment': comment['comment'],
            'Published_At': comment['published_at'],
            'Likes': comment['like_count'],
            'Replies': comment['reply_count'],
            'Sentiment': sentiment,
            'Comment_Length': len(comment['comment'])
        })

    df = pd.DataFrame(results)
    return df, video_info

"""### Cell 9: Visualization Functions"""

class SentimentVisualizer:
    """Visualization class for sentiment analysis results"""

    def __init__(self, df):
        self.df = df
        self.colors = Config.COLORS

    def plot_sentiment_distribution(self):
        """Plot sentiment distribution"""
        plt.figure(figsize=Config.FIGSIZE)

        sentiment_counts = self.df['Sentiment'].value_counts()

        plt.subplot(2, 2, 1)
        sentiment_counts.plot(kind='bar', color=self.colors)
        plt.title('Sentiment Distribution')
        plt.xlabel('Sentiment')
        plt.ylabel('Count')
        plt.xticks(rotation=45)

        plt.subplot(2, 2, 2)
        plt.pie(sentiment_counts.values, labels=sentiment_counts.index,
                autopct='%1.1f%%', colors=self.colors)
        plt.title('Sentiment Distribution (Pie Chart)')

        plt.tight_layout()
        plt.show()

    def plot_sentiment_over_time(self):
        """Plot sentiment over time"""
        plt.figure(figsize=Config.FIGSIZE)

        # Convert timestamp to datetime
        self.df['Published_At'] = pd.to_datetime(self.df['Published_At'])

        # Group by date and sentiment
        daily_sentiment = self.df.groupby([
            self.df['Published_At'].dt.date, 'Sentiment'
        ]).size().unstack(fill_value=0)

        daily_sentiment.plot(kind='line', marker='o', figsize=(12, 6))
        plt.title('Sentiment Trends Over Time')
        plt.xlabel('Date')
        plt.ylabel('Number of Comments')
        plt.legend(title='Sentiment')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    def plot_engagement_analysis(self):
        """Plot engagement analysis"""
        plt.figure(figsize=Config.FIGSIZE)

        plt.subplot(2, 2, 1)
        self.df.boxplot(column='Likes', by='Sentiment', ax=plt.gca())
        plt.title('Likes by Sentiment')
        plt.suptitle('')

        plt.subplot(2, 2, 2)
        self.df.boxplot(column='Comment_Length', by='Sentiment', ax=plt.gca())
        plt.title('Comment Length by Sentiment')
        plt.suptitle('')

        plt.subplot(2, 2, 3)
        avg_likes = self.df.groupby('Sentiment')['Likes'].mean()
        avg_likes.plot(kind='bar', color=self.colors)
        plt.title('Average Likes by Sentiment')
        plt.xticks(rotation=45)

        plt.subplot(2, 2, 4)
        avg_length = self.df.groupby('Sentiment')['Comment_Length'].mean()
        avg_length.plot(kind='bar', color=self.colors)
        plt.title('Average Comment Length by Sentiment')
        plt.xticks(rotation=45)

        plt.tight_layout()
        plt.show()

    def generate_word_cloud(self):
        """Generate word clouds for each sentiment"""
        sentiments = self.df['Sentiment'].unique()

        fig, axes = plt.subplots(1, len(sentiments), figsize=(15, 5))
        if len(sentiments) == 1:
            axes = [axes]

        for i, sentiment in enumerate(sentiments):
            comments = self.df[self.df['Sentiment'] == sentiment]['Comment']
            text = ' '.join(comments)

            wordcloud = WordCloud(width=400, height=200,
                                background_color='white',
                                colormap='viridis').generate(text)

            axes[i].imshow(wordcloud, interpolation='bilinear')
            axes[i].set_title(f'{sentiment} Comments')
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

    def generate_summary_report(self, video_info=None):
        """Generate a summary report"""
        total_comments = len(self.df)
        sentiment_counts = self.df['Sentiment'].value_counts()

        print("📊 SENTIMENT ANALYSIS REPORT")
        print("=" * 50)

        if video_info:
            print(f"📺 Video: {video_info['title']}")
            print(f"📺 Channel: {video_info['channel']}")
            print(f"👁️  Views: {video_info['view_count']}")
            print()

        print(f"💬 Total Comments Analyzed: {total_comments}")
        print(f"📅 Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        print("📈 Sentiment Distribution:")
        for sentiment, count in sentiment_counts.items():
            percentage = (count / total_comments) * 100
            print(f"   {sentiment}: {count} ({percentage:.1f}%)")

        print()
        print("🔍 Key Insights:")

        # Most liked comment by sentiment
        for sentiment in sentiment_counts.index:
            most_liked = self.df[self.df['Sentiment'] == sentiment].nlargest(1, 'Likes')
            if not most_liked.empty:
                print(f"   Most liked {sentiment.lower()} comment ({most_liked.iloc[0]['Likes']} likes):")
                print(f"   \"{most_liked.iloc[0]['Comment'][:100]}...\"")

        # Average engagement
        avg_likes = self.df.groupby('Sentiment')['Likes'].mean()
        print(f"\n📊 Average Likes by Sentiment:")
        for sentiment, avg in avg_likes.items():
            print(f"   {sentiment}: {avg:.1f} likes")

### Cell 10: Main Execution

def main():
    """Main execution function"""

    # Example YouTube video URL (replace with your desired video)
    video_url = input("Enter YouTube video URL: ")
    max_comments = int(input("Enter maximum number of comments to analyze (default 100): ") or 100)

    print(f"\n🚀 Starting sentiment analysis for: {video_url}")
    print(f"📊 Maximum comments to analyze: {max_comments}")

    # Analyze video
    result = analyze_youtube_video(video_url, max_comments)

    if result is None:
        print("❌ Analysis failed")
        return

    df, video_info = result

    # Create visualizer
    visualizer = SentimentVisualizer(df)

    # Generate visualizations
    print("\n📊 Generating visualizations...")
    visualizer.plot_sentiment_distribution()
    visualizer.plot_engagement_analysis()
    visualizer.generate_word_cloud()

    # Generate summary report
    visualizer.generate_summary_report(video_info)

    # Save results
    output_file = f"sentiment_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(output_file, index=False)
    print(f"\n💾 Results saved to: {output_file}")

    return df, video_info

"""### Cell 11: Interactive Analysis"""

def interactive_analysis():
    """Interactive analysis function"""

    print("🎯 Tamil-English Sentiment Analysis Tool")
    print("=" * 50)

    while True:
        print("\nChoose an option:")
        print("1. Analyze YouTube video")
        print("2. Test with custom text")
        print("3. Batch analyze multiple videos")
        print("4. Exit")

        choice = input("\nEnter your choice (1-4): ")

        if choice == '1':
            video_url = input("Enter YouTube video URL: ")
            max_comments = int(input("Enter max comments (default 100): ") or 100)

            result = analyze_youtube_video(video_url, max_comments)
            if result:
                df, video_info = result
                visualizer = SentimentVisualizer(df)
                visualizer.plot_sentiment_distribution()
                visualizer.generate_summary_report(video_info)

        elif choice == '2':
            text = input("Enter text to analyze: ")
            sentiment = analyzer.classify_sentiment(text)
            print(f"Sentiment: {sentiment}")

        elif choice == '3':
            urls = input("Enter YouTube URLs (comma-separated): ").split(',')
            all_results = []

            for url in urls:
                url = url.strip()
                print(f"\nAnalyzing: {url}")
                result = analyze_youtube_video(url, 50)
                if result:
                    df, video_info = result
                    df['Video_Title'] = video_info['title'] if video_info else 'Unknown'
                    all_results.append(df)

            if all_results:
                combined_df = pd.concat(all_results, ignore_index=True)
                print(f"\nCombined analysis of {len(combined_df)} comments")
                print(combined_df['Sentiment'].value_counts())

        elif choice == '4':
            print("👋 Goodbye!")
            break

        else:
            print("❌ Invalid choice. Please try again.")
    # Run interactive analysis
interactive_analysis()

"""### Cell 12: Batch Processing Functions"""

def batch_analyze_videos(video_urls, max_comments_per_video=50):
    """Batch analyze multiple videos"""

    all_results = []

    for i, url in enumerate(video_urls, 1):
        print(f"\n🔄 Processing video {i}/{len(video_urls)}: {url}")

        result = analyze_youtube_video(url, max_comments_per_video)
        if result:
            df, video_info = result
            df['Video_URL'] = url
            df['Video_Title'] = video_info['title'] if video_info else 'Unknown'
            df['Video_Channel'] = video_info['channel'] if video_info else 'Unknown'
            all_results.append(df)

        time.sleep(2)  # Rate limiting

    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)

        print(f"\n📊 Batch Analysis Complete!")
        print(f"Total videos analyzed: {len(video_urls)}")
        print(f"Total comments analyzed: {len(combined_df)}")

        # Save combined results
        output_file = f"batch_sentiment_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
        combined_df.to_csv(output_file, index=False)
        print(f"💾 Results saved to: {output_file}")

        # Generate summary
        print("\n📈 Overall Sentiment Distribution:")
        print(combined_df['Sentiment'].value_counts())

        # Per-video summary
        print("\n📺 Per-Video Summary:")
        video_summary = combined_df.groupby('Video_Title')['Sentiment'].value_counts().unstack(fill_value=0)
        print(video_summary)

        return combined_df

    return None

# Example usage:
# video_urls = [
#     "https://youtu.be/VIDEO_ID_1",
#     "https://youtu.be/VIDEO_ID_2",
#     "https://youtu.be/VIDEO_ID_3"
# ]
# batch_results = batch_analyze_videos(video_urls)

print("✅ Tamil-English Sentiment Analysis Tool Ready!")
print("📋 Available functions:")
print("   - main(): Run complete analysis")
print("   - interactive_analysis(): Interactive mode")
print("   - batch_analyze_videos(): Batch processing")
print("   - test_model(): Test with sample texts")